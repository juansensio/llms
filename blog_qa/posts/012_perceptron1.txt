
El Perceptr贸n
En los posts anteriores hemos ido viendo los diferentes elementos que necesitamos para desarrollar e implementar nuestros algoritmos de Inteligencia Artificial. Empezamos aprendiendo Python, el lenguaje de programaci贸n m谩s utilizando para el an谩lisis de datos. Seguimos con la librer铆a Numpy, muy 煤til para el c谩lculo num茅rico. Tambi茅n hemos hablado de los conceptos matem谩ticos fundamentales que tenemos que conocer en nuestro viaje por el mundo del Machine Learning: lgebra lineal, probabilidad y c谩lculo num茅rico. En este post vamos a utilizar todos estos conceptos para implementar nuestro primer algoritmo de IA. el Perceptr贸n.
Redes Neuronales biol贸gicas
El Perceptr贸n es la unidad de computaci贸n b谩sica utilizada en las redes neuronales, aunque tambi茅n puede usarse por si solo como algoritmo de IA en algunos casos. Est谩 inspirado en el funcionamiento de las neuronas biol贸gicas, y fue propuesto en 1957 por Frank Rosenblatt.

Una neurona est谩 compuesta de un cuerpo celular que contiene el n煤cleo y la mayor铆a de los componentes complejos de la c茅lula, muchas extensiones ramificadas llamadas dendritas y una extensi贸n muy larga llamada ax贸n. Cerca de su extremidad, el ax贸n se divide en ramas llamadas telodendria, y en la punta de estas ramas encontramos estructuras min煤sculas llamadas terminales sin谩pticas (o simplemente sinapsis), que est谩n conectadas a las dendritas o cuerpos celulares de otras neuronas. Las neuronas biol贸gicas producen impulsos el茅ctricos cortos llamados potenciales de acci贸n (o simplemente se帽ales) que viajan a lo largo de los axones y hacen que las sinapsis liberen se帽ales qu铆micas llamadas neurotransmisores. Cuando una neurona recibe una cantidad suficiente de estos neurotransmisores en unos pocos milisegundos, dispara sus propios impulsos el茅ctricos. Aunque las neuronas individuales parecen comportarse de manera bastante simple, pueden conectarse a miles de otras neuronas formando redes complejas que pueden realizar tareas complejas.
El Perceptr贸n
El Perceptr贸n es una versi贸n simplificada de la neurona que calcula la suma ponderada de todas sus entradas y despu茅s aplica una funci贸n de activaci贸n para dar el resultado


\hat{y} = f(\mathbf{w} \cdot \mathbf{x}) = f(w_0 + w_1 x_1 + ... + w_m x_m)





























































d贸nde 
\hat{y}








 es la salida del Perceptr贸n, 
\mathbf{w}






 son sus par谩metros (tambi茅n llamados los pesos), 
\mathbf{x}






 son las entradas y 
f






 es la funci贸n de activaci贸n. A continuaci贸n puedes ver un espuema del Perceptr贸n, puedes compararlo con el esquema de la neurona biol贸gica para evaluar sus parecidos y diferencias.

Regresi贸n Lineal
Existen varias tareas que podemos llevar a cabo con un Perceptr贸n. En este post vamos a ver una de ellas: regresi贸n lineal. En este tipo de tarea, como en cualquier tarea de regresi贸n, queremos obtener un modelo que se ajuste de la mejor forma posible a un conjunto de datos determinado. En el caso de la regresi贸n lineal, este modelo ser谩 una l铆nea recta y el Perceptr贸n, utilizando una funci贸n de activaci贸n lineal, 
f(x) = x















, es de hecho capaz de llevar a cabo esta tarea.
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(-10, 10)
y = x

plt.plot(x, y)
plt.grid(True)
plt.xlabel('x', fontsize=14)
plt.ylabel('f(x) = x', fontsize=14)
plt.title('Funci贸n de activaci贸n lineal', fontsize=14)
plt.plot(x, np.zeros(len(x)), '--k')
plt.show()



 Utilizar una funci贸n de activaci贸n lineal es como no utilizar ninguna funci贸n de activaci贸n, ya que la salida del Perceptr贸n ser谩 directamente el producto de las entradas por los pesos, 
\hat{y} = \mathbf{w} \cdot \mathbf{x}
















.

Para ilustrar c贸mo funciona el Perceptr贸n para tareas de regresi贸n lineal, vamos a utilizar el siguiente dataset sint茅tico.
np.random.seed(42)

x = np.random.rand(20)
y = 2*x + (np.random.rand(20)-0.5)*0.5

plt.plot(x, y, "b.")
plt.xlabel("$x_1$", fontsize=14)
plt.ylabel("$y$", rotation=0, fontsize=14)
plt.grid(True)
plt.show()


Para empezar de manera sencilla, vamos a considerar que s贸lo tenemos una caracter铆stica que usaremos como entrada para nuestro Perceptr贸n, 
x_1








. El objetivo es que el Perceptr贸n, al recibir cada uno de estos valores, nos de como salida un valor lo m谩s cercano posible a 
y






. En este caso, al tener una sola caracter铆stica por elemento, nuestro Perceptr贸n sigue la siguiente expresi贸n


 \hat{y} = \mathbf{w} \cdot \mathbf{x} = w_0 + w_1 x_1





































En el caso en que 
w_0 = 0











 y 
w_1 = 2












, 
\hat{y} = 2 x














, lo cual podemos representar como una recta. En el caso de querer conocer el valor de 
y






 para un nuevo elemento, por ejemplo 
x_1=0.5


















, simplemente tenemos que calcular su valor haciendo regresi贸n a nuestra recta, en este caso 
\hat{y} = 1












. Un ejemplo de aplicaci贸n de este tipo de modelos de regresi贸n es la predicci贸n del precio de un inmueble (
\hat{y}








) dadas una serie de caracter铆sticas como su localizaci贸n (
x_1








), metros cuadrados (
x_2








), n煤mero de habitaciones (
x_3








), etc. Tener un buen modelo de regresi贸n para esta aplicaci贸n nos puede ayudar a saber si una casa en venta est谩 por encima o por debajo de su valor real ayud谩ndonos a tomar mejores decisiones de inversi贸n.
plt.plot(x, y, "b.")
plt.plot(x, 2*x, 'k')
plt.plot(0.5, 2*0.5, 'sr')
plt.xlabel("$x_1$", fontsize=14)
plt.ylabel("$y$", rotation=0, fontsize=14)
plt.grid(True)
plt.show()


En este ejemplo sabemos que los pesos de nuestro modelo son 
w_0 = 0











 y 
w_1 = 2












 ya que son los mismos utilizados para generar los datos. Sin embargo, nuestro objetivo ser谩 el de encontrar estos valores utilizando el algoritmo de descenso por gradiente.
Entrenando el Perceptr贸n
En este post ya vimos una introducci贸n al algoritmo de descenso por gradiente. Aqu铆 vamos a aplicarlo para entrenar el Perceptr贸n. Nuestro objetivo es el de encontrar los pesos de nuestro modelo, 
\mathbf{w}






, que minimicen una funci贸n de p茅rdida. Esta funci贸n nos da una medida del error que comete nuestro modelo al predecir los datos de nuestro dataset. En tareas de regresi贸n, la funci贸n de p茅rdida m谩s utilizada es el error medio cuadr谩tico o mean square loss (MSE) en ingl茅s.


 MSE(\hat{y},y) = \frac{1}{N} \sum^{N}\_{j=1} (\hat{y}^{(j)} - y^{(j)})^2










































































d贸nde 
N






 es el n煤mero de muestras del dataset. Esta funci贸n recibe las predicciones de nuestro modelo para los datos del dataset, 
\hat{y}^{(j)}
















, y las compara con los valores reales (ground truth), 
y^{(j)}














, calculando el valor medio de la diferencia para cada elemento del dataset elevada al cuadrado.

El algoritmo de descenso por gradiente se implementa de la forma siguiente:

Calcular la salida del modelo, 
\hat{y}








.
Calcular la derivada de la funci贸n de p茅rdida con respecto a los par谩metros del modelo, 
\frac{\partial MSE}{\partial w} = \frac{2}{N} \frac{\partial \hat{y}}{\partial w} (\hat{y} - y) 






























































 d贸nde 
\frac{\partial \hat{y}}{\partial w} = x


























.
Actualizar los par谩metros, 
w \leftarrow w - \eta \frac{\partial MSE}{\partial w}
































, d贸nde 
\eta






 es el learning rate.
Repetir hasta converger.

Inicializaremos nuestro modelo con unos valores aleatorios para los pesos, e iremos actualizando sus valores de manera iterativa en la direcci贸n de pendiente negativa de la funci贸n de p茅rdida. El algoritmo termina cuando observemos que el valor de la funci贸n de p茅rdida deja de mejorar (indicando que habremos llegado a un m铆nimo local de nuestra funci贸n de p茅rdida).
def gradient(w, x, y):
    # calculamos la derivada de la funci贸n de p茅rdida
    # con respecto a los par谩mteros `w`
    dldw = x*w - y
    dydw = x
    dldw = dldw*dydw
    return np.mean(2*dldw)

def cost(y, y_hat):
    # calculamos la funci贸n de p茅rdida
    return ((y_hat - y)**2).mean()

def solve(epochs = 29, w = 1.2, lr = 0.2):
    # iteramos un n煤mero determinado de `epochs`
    # por cada epoch, calculamos gradientes y
    # actualizamos los pesos
    weights = [(w, gradient(w, x, y), cost(x*w, y))]
    for i in range(1,epochs+1):
        dw = gradient(w, x, y)
        w = w - lr*dw
        weights.append((w, dw, cost(x*w, y)))
    return weights



  Your browser does not support the video tag.

Como puedes observar, el algoritmo va actualizando los pesos del modelo en aquella direcci贸n que minimize la funci贸n de p茅rdida (figura de la izquierda). Una vez encontramos los valores 贸ptimos, podemos representar nuestro modelo (figura de la derecha) y utilizarlo para generar nuevas predicciones.
# 煤tlimo peso obtenido

w = weights[-1][0]
w

1.8648014707003089

# nueva predicci贸n

x_new = 0.5
y_new = w*x_new
y_new

0.9324007353501544

plt.plot(x, y, "b.")
plt.plot(x, w*x, 'k')
plt.plot(0.5, w*0.5, 'sr')
plt.xlabel("$x_1$", fontsize=14)
plt.ylabel("$y$", rotation=0, fontsize=14)
plt.grid(True)
plt.show()


Como podr谩s intuir, este sencillo modelo s贸lo ser谩 煤til en el caso de tener un conjunto de datos que puedan ser representados por un modelo lineal. Cuando 茅ste no sea el caso (lo cual ocurrir谩 en la mayor铆a de casos) necesitaremos modelos m谩s potentes (que veremos en futuros posts). Esta es la principal limitaci贸n del Perceptr贸n.
m = 100
x = 6 * np.random.rand(m, 1)
y = (x - 3)**2 + np.random.randn(m, 1)

weights = solve(lr=0.01)
w = weights[-1][0]

plt.plot(x, y, "b.")
plt.plot(x, w*x, '-k')
plt.xlabel("$x_1$", fontsize=14)
plt.ylabel("$y$", rotation=0, fontsize=14)
plt.grid(True)
plt.show()


Resumen
En este post hemos introducido nuestro primer modelo de Machine Learning: el Perceptr贸n. Este modelo est谩 inspirado en el funcionamiento de las neuronas biol贸gicas, recibiendo se帽ales de entradas, llevando a cabo una serie de c谩lculos y dando un resultado como salida. En el caso del Perceptr贸n, calculamos la suma ponderada de todas sus entradas y luego aplicamos una funci贸n de activaci贸n sobre el resultado para calcular la salida. La aplicaci贸n m谩s sencilla del Perceptr贸n es la regresi贸n lineal, d贸nde intentaremos encontrar una l铆nea que represente un conjunto de datos determinados. Para encontrar los pesos del modelo que mejor representen el dataset, usamos el algoritmo de descenso por gradiente. En este algoritmo, calcularemos la salida del modelo, derivamos la funci贸n de p茅rdida con respecto a los pesos del modelo los cuales actualizamos en la direcci贸n de derivada negativa para minimizar de manera iterativa la funci贸n de p茅rdida.

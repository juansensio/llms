
Physics-Based Deep Learning
Introducci√≥n
Este es el primero en una serie de posts en los que aprenderemos sobre PBDL: Physics-Based Deep Learning, o el uso del Deep Learning (redes neuronales) para simulaci√≥n f√≠sica. En concreto, nos centraremos en CFD: Computational Fluid Dynamics, el campo de la f√≠sica que se enfoca en la simulaci√≥n de fluidos para aplicaciones de aerodin√°mica, combusti√≥n, etc. Te advierto que nos vamos a alejar del machine learning tradicional para explorar un nuevo campo, el del uso de las redes neuronales para aproximar soluciones a ecuaciones diferenciales. Es posible que en algunos momentos te preguntes: ¬øes esto realmente machine learning? Te entiendo. A√∫n as√≠, creo firmemente que el campo del PBDL revolucionar√° la manera en la que simulamos la naturaleza en los pr√≥ximos a√±os, de la misma manera que el Deep Learning ha revolucionado (y lo sigue haciendo) tantos otros campos de la ciencia, como por ejemplo el plegado de proteinas.
El campo del PBDL es una disciplina relativamente nueva e inexplorada que se basa el uso de redes neuronales para sustituir (o complementar) m√©todos num√©ricos "tradicionales" utilizados desde hace a√±os para simular los diferentes procesos f√≠sicos que rigen nuestra naturaleza (desde el comportamiento de nuestra atm√≥sfera hasta el movimiento de estrellas y galaxias). Estos procesos pueden ser descritos, en la mayor√≠a de ocasiones, mediante ecuaciones matem√°ticas. Resolver estas ecuaciones nos permite calcular, por ejemplo, la distribuci√≥n de presi√≥n sobre una superficie aerodin√°mica (lo cual es muy √∫til a la hora de dise√±ar aviones m√°s eficientes, entre muchas otras aplicaciones). Sin embargo, como te podr√°s imaginar, estas ecuaciones suelen ser muy dif√≠ciles de resolver y, en la mayor√≠a de situaciones, ni siquiera pueden ser resueltas de manera anal√≠tica. Es aqu√≠ donde entran en juego los m√©todos num√©ricos, t√©cnicas que nos permiten aproximar soluciones a estas ecuaciones que si bien no son exactas son lo suficientemente precisas para su uso en aplicaciones reales. Tradicionalmente, m√©todos num√©ricos de este estilo requieren de grandes recursos computacionales (es por este motivo que tenemos "superordenadores"). Por lo que cualquier avance en el campo que nos permita encontrar soluciones m√°s r√°pidas y baratas supone una revoluci√≥n. Creo que el campo del PBDL ser√° la siguiente revoluci√≥n en este campo. De hecho, este fue el motivo por el que me adentr√© en el mundo del Deep Learning, persiguiendo la idea de que usar redes neuronales para aproximar soluciones a ecuaciones diferenciales pod√≠a ser una buena idea.

Recientemente se ha publicado este libro sobre PBDL. No dudes en consultarlo para aprender m√°s !

Computational Fluid Dynamics
Dentro del gran abanico de aplicaciones de la f√≠sica computacional, la mec√°nica de fluidos computacional se encarga del estudio del comportamiento de fluidos, principalmente mediante la resoluci√≥n de las ecuaciones de Navier-Stokes. Esto tiene un uso muy importante en el dise√±o de aeronaves, coches (muy importante en coches el√©ctricos), previsi√≥n meteorol√≥gica y an√°lisis de la evoluci√≥n de contaminantes, etc.

Como ya he comentado anteriormente, resolver estas ecuaciones de manera anal√≠tica es imposible y su resoluci√≥n num√©rica require de grandes recursos computacionales. A√∫n as√≠, cada vez es m√°s extendido su uso. En el caso del sector aeron√°utico la alternativa es el uso de t√∫neles de viento, lo cual es todav√≠a m√°s caro y lento. Poder dise√±ar veh√≠culos con software de dise√±o 3d por ordenador, simular su comportamiento en varias condiciones e iterar su dise√±o hasta encontrar la geometr√≠a √≥ptima en entornos virtuales es una gran ventaja. Creo que el uso del Deep Learning para CFD supondr√° una revoluci√≥n y acelerar√°, a la vez que abaratar√°, todo este proceso dando como resultado veh√≠culos m√°s eficientes, que viajen m√°s r√°pido consumiendo y contaminando menos.

Si quieres aprender m√°s sobre CFD te recomiendo echarle un vistazo a mi tesis doctoral ü§ó

Leyes de Conservaci√≥n
Iniciamos nuestro viaje en el campo del PBDL viendo el primer ejemplo de ecuaci√≥n diferencial que vamos a resolver, primero con m√©todos num√©ricos tradicionales y luego con redes neuronales. Nuestro objetivo final es el de resolver las ecuaciones de Navier-Stokes, las cuales se agrupan dentro de las leyes de conservaci√≥n. Una ley de conservaci√≥n explica la evoluci√≥n en el tiempo, 
t






, y el espacio, 
x






, de una variable conservativa. Un ejemplo muy simple es la evoluci√≥n de la masa, 
m






, de un fluido en un dominio unidimensional.


m(t)  = \int_{x_1}^{x_2} \rho(x, t) dx










































En este caso, la magnitud conservativa es la densidad, 
\rho






. Asumiendo que ni se crea ni se destruye masa, su evoluci√≥n, 
m(t)












, se deber√° √∫nicamente a la cantidad de fluido que est√© entrando y saliendo el dominio a trav√©s de la entrada, en 
x_1








, y la salida, en 
x_2








 (lo que llamamos el flujo, 
F






).


\frac{\partial}{\partial t} \int_{x_1}^{x_2} \rho(x, t) dx = F_1(t) - F_2(t)































































De conocer la velocidad del fluido, podr√≠amos calcular estos flujos como 
f(\rho(x,t)) = u(x,t)\rho(x,t)



































.


\frac{\partial}{\partial t} \int_{x_1}^{x_2} \rho(x, t) dx = f(\rho(x_1,t)) - f(\rho(x_2,t))









































































Manipulando esta ecuaci√≥n podemos derivar la siguiente ecuaci√≥n diferencial


\frac{\partial}{\partial t} \int_{x_1}^{x_2} \rho(x, t) dx = - \int_{x_1}^{x_2} \frac{\partial}{\partial x} f(\rho(x,t)) dx

















































































Que, reordenando, quedar√≠a como


\int_{x_1}^{x_2} \left[ \frac{\partial}{\partial t} \rho(x, t) + \frac{\partial}{\partial x} f(\rho(x,t)) \right] dx = 0














































































Y, debido a que esta integral tiene que ser cero para todos los valores de 
x_1








 y 
x_2











\frac{\partial}{\partial t} \rho(x, t) + \frac{\partial}{\partial x} f(\rho(x,t)) = 0





















































Esta es la formulaci√≥n b√°sica de la ecuaci√≥n de conservaci√≥n de la masa. La forma general para una variable conservativa cualquiera se podr√≠a escribir como


\frac{\partial}{\partial t} \phi(x, t) + \frac{\partial}{\partial x} f(\phi(x,t)) = 0





















































O, de manera m√°s compacta, simplemente


\phi_t + f(\phi)_x = 0



























M√©todos de Vol√∫menes Finitos
Existen multitud de m√©todos num√©ricos para resolver ecuaciones diferenciales (diferencias finitas, elementos finitos, m√©todos de Galerkin, ...). Para la resoluci√≥n de ecuaciones de conservaci√≥n, lo que nos interesa a nosotros, el uso de m√©todos de vol√∫menes finitos es el m√°s extendido debido a sus propiedades favorables.
Este m√©todo est√° basado en la discretizaci√≥n del dominio de inter√©s en el que queremos resolver nuestra ecuaci√≥n en una serie de celdas en las cuales representaremos nuestra soluci√≥n de manera promediada.


\overline{\phi}(x^k, t) = \frac{1}{h^k} \int_k \phi(x, t) dx


























































Y para satisfacer la ecuaci√≥n diferencial se debe cumplir que


h^k \frac{\partial \overline{\phi}(x^k)}{\partial t} = f(x^{k - 1/2},t) - f(x^{k + 1/2},t)





















































































Lo cual requerir√° el c√°lculo del flujo 
f(x^{k \pm 1/2}) = F(\phi^k, \phi^{k \pm 1})















































.
La ecuaci√≥n de convecci√≥n 1D
Vamos a ver nuestro primer ejemplo de ecuaci√≥n diferencial: la ecuaci√≥n de convecci√≥n 1D.


\phi_t + u \phi_x = 0























En este caso, la variable conservativa es 
\phi(x, t)
















 y el flujo es 
f(\phi(x,t)) = u \phi(x, t)






























 donde 
u






 es la velocidad, un valor escalar constante. Esta ecuaci√≥n es muy √∫til por varios motivos. En primer lugar, considerando condiciones de contorno peri√≥dicas, tiene soluci√≥n anal√≠tica


\phi(x,t) = \phi_0(x - ut)

































donde 
\phi_0








 es la condici√≥n inicial. A grandes rasgos, la condici√≥n inicial se propagar√° en 
x






 a la velocidad 
u






. Esto convierte a la ecuaci√≥n de convecci√≥n 1D como un perfecto benchmark para probar diferentes m√©todos num√©ricos.
import numpy as np
import math
import matplotlib.pyplot as plt

x = np.linspace(0,1,100)
p = np.sin(2*math.pi*x)

plt.figure(dpi=100)
plt.plot(x, p)
plt.grid(True)
plt.xlabel('x')
plt.ylabel('$\phi_0$')
plt.show()


from matplotlib import animation, rc
rc('animation', html='html5')

def update(i):
    ax.clear()
    ax.plot(x, ps[i], "b")
    ax.set_xlabel('x')
    ax.set_ylabel('$\phi$')
    ax.set_title(f't = {ts[i]:.3f}')
    ax.grid(True)
    return ax

def compute_sol(x, u, t):
    return np.sin(2*math.pi*(x - u*t))

u = 1
ts = np.linspace(0,1,50)
ps = []
for t in ts:
    p = compute_sol(x, u, t)
    ps.append(p)

fig = plt.figure(dpi=100)
ax = plt.subplot(1,1,1)
anim = animation.FuncAnimation(fig, update, frames=len(ps), interval=200)
plt.close()

anim



  Your browser does not support the video tag.

Resoluci√≥n con m√©todos de vol√∫menes finitos
Lo primero que necesitamos para resolver la ecuaci√≥n es discretizar nuestro dominio en un conjunto de celdas.
L, N, u = 1., 10, 1.
dx, dt = L / N, (L / N) / u

f = np.linspace(0,L,N+1) # caras
c = np.linspace(0.5*dx,L-0.5*dx,N) # celdas

plt.figure(figsize=(10,1))
plt.plot(f, np.zeros(N+1), '.k')
plt.plot(c, np.zeros(N), '+r')
plt.grid(True)
plt.xlabel('x')
plt.show()


Ahora, nuestra condici√≥n inicial estar√° definida en cada celda como el valor en su centroide (t√©cnicamente deber√≠amos calcular su integral volum√©trica, pero esto no es necesario para nuestro objetivo).
p0 = np.sin(2*math.pi*c)

plt.figure(dpi=100)
plt.plot(c, p0, '+r')
a, b = [0], [0]
for i in range(N):
    a += [c[i] - 0.5*dx, c[i], c[i] + 0.5*dx]
    b += [p0[i], p0[i], p0[i]]
a += [1]
b += [0]
plt.plot(a, b, '-r')
plt.grid(True)
plt.xlabel('x')
plt.ylabel('$\phi_0$')
plt.show()


Como hemos visto antes, para calcular la evoluci√≥n temporal de la variable conservativa necesitamos calcular su flujo a trav√©s de sus caras. Existen m√∫ltiples esquemas num√©ricos para aproximar el valor de la variable conservativa en la cara. Un par de esquemas simples son los conocidos como upwind (UDS), que usa el valor aguas arriba, y central (CDS) que usa el valor medio entre los valores a cada lado de la cara.
import numpy as np

def compute_flux(p, u, scheme='uds'):
    N = len(p)
    f = np.ones(N+1)
    for i in range(N+1):
        l, r = i-1, i
        if i == 0:
            l = N - 1
        elif i == N:
            r = 0
        if scheme == 'uds':
            f[i] = p[l] if u > 0 else p[r]
        elif scheme == 'cds':
            f[i] = 0.5*(p[l] + p[r])
        else:
            raise 'invalid scheme'
    return f

pf = compute_flux(p0, u, 'uds')

plt.figure(dpi=100)
plt.plot(c, p0, '+r')
plt.plot(a, b, '-r')
plt.plot(f, pf, '.k')
plt.grid(True)
plt.xlabel('x')
plt.ylabel('$\phi_0$')
plt.show()


pf = compute_flux(p0, u, 'cds')

plt.figure(dpi=100)
plt.plot(c, p0, '+r')
plt.plot(a, b, '-r')
plt.plot(f, pf, '.k')
plt.grid(True)
plt.xlabel('x')
plt.ylabel('$\phi_0$')
plt.show()


Para cada celda, calcularemos la variaci√≥n de la magnitud conservativa como la cantidad que sale por la cara de la derecha menos la que entra por la cara izquierda (teniendo en cuenta el signo).
def sum_fluxes(pf):
    N = len(pf) - 1
    r = np.zeros(N)
    for i in range(N):
        r[i] = pf[i] - pf[i+1]
    return r

Por √∫ltimo, podemos actualizar el resultado usando un esquema de integraci√≥n temporal, en este caso aplicamos un esquema sencillo de primer orden.


    \frac{\partial \phi}{\partial t} \approx \frac{\phi^{n+1} - \phi^n}{t^{n+1} - t^{n}}

























































def update_p(p0, r, dt, dx):
    N = len(p0)
    p = np.zeros(N)
    for i in range(N):
        p[i] = p0[i] + (dt/dx)*r[i]
    return p

p0 = np.sin(2*math.pi*c)
pf = compute_flux(p0, u)
r = sum_fluxes(pf)
p = update_p(p0, r, dt, dx)

plt.figure(dpi=100)
plt.plot(c, p0, '+-r')
plt.plot(f, pf, '.k')
plt.plot(c, p, '+-g')
plt.grid(True)
plt.xlabel('x')
plt.ylabel('$\phi_0$')
plt.show()


pf = compute_flux(p, u)
r = sum_fluxes(pf)
p1 = update_p(p, r, dt, dx)

plt.figure(dpi=100)
plt.plot(c, p0, '+-r')
plt.plot(f, pf, '.k')
plt.plot(c, p, '+-g')
plt.plot(c, p1, '+-b')
plt.grid(True)
plt.xlabel('x')
plt.ylabel('$\phi_0$')
plt.show()


def run_sim(cfl = 1., scheme='uds'):
    p0 = np.sin(2*math.pi*c)
    ps = [p0]
    pa = [p0]
    ts = [0]
    t = 0
    dt = cfl * (L / N) / u
    while t + dt < 1.:
        t += dt
        pf = compute_flux(p0, u, scheme)
        r = sum_fluxes(pf)
        p = update_p(p0, r, dt, dx)
        ps.append(p)
        pa.append(compute_sol(c, u, t))
        ts.append(t)
        p0 = p.copy()
    return ps, pa, ts

def update(i):
    ax.clear()
    ax.plot(c, pa[i], "-b")
    ax.plot(c, ps[i], "+-r")
    ax.set_xlabel('x')
    ax.set_ylabel('$\phi$')
    ax.set_title(f't = {ts[i]:.3f}')
    ax.grid(True)
    ax.set_ylim(-1.1,1.1)
    return ax

ps, pa, ts = run_sim()

fig = plt.figure(dpi=100)
ax = plt.subplot(1,1,1)
anim = animation.FuncAnimation(fig, update, frames=len(ps), interval=200)
plt.close()

anim



  Your browser does not support the video tag.

En este caso particular, resolver la ecuaci√≥n de convecci√≥n con un esquema UDS en una malla regular con un incremento temporal 
dt = \frac{dx}{u}






















 y velocidad constante es equivalente a la soluci√≥n anal√≠tica. Esto es debido a que, bajo estas condiciones especiales, el flujo calculado es exacto. Esto, sin embargo, nunca ser√° el caso ya que trabajaremos con mallas irregulares, la velocidad del fluido ser√° variables, etc. En estos casos, el uso de un esquema UDS resulta en una soluci√≥n artificalmente viscosa.
ps, pa, ts = run_sim(cfl=0.5)

fig = plt.figure(dpi=100)
ax = plt.subplot(1,1,1)
anim = animation.FuncAnimation(fig, update, frames=len(ps), interval=200)
plt.close()

anim



  Your browser does not support the video tag.

El esquema CDS, por otro lado, acumular√° errores debido a la aproximaci√≥n que resultan en la divergencia de la simulaci√≥n.
ps, pa, ts = run_sim(cfl=0.5, scheme="cds")

fig = plt.figure(dpi=100)
ax = plt.subplot(1,1,1)
anim = animation.FuncAnimation(fig, update, frames=len(ps), interval=200)
plt.close()

anim



  Your browser does not support the video tag.

Para aliviar estos problemas se suele recurrir a esquemas num√©ricos de alto nivel (como por ejemplo los esquemas Runge Kutta para la integraci√≥n temporal), el uso de mallas muy densas y saltos temporales muy peque√±os con el objetivo de reducir al m√≠nimo los errores de aproximaci√≥n. Este es el principal motivo por el que resolver ecuaciones diferenciales con m√©todos num√©ricos suele requerir de grandes requisitos computacionales. La pregunta ahora es: ¬øpodemos mejorar esto con redes neuronales? La respuesta es ¬°S√ç!
PINNs
El campo del PBDL abarca diferentes m√©todos. Vamos a empezar introduciendo el uso de PINNs, o Physics informed Neural Networks. En este caso, usaremos redes neuronales para aproximar la soluci√≥n a ecuaciones diferenciales usando la misma ecuaci√≥n como objetivo de aprendizaje (funci√≥n de p√©rdida). As√≠ pues, nuestra red neuronal recibir√° a la entrada un conjunto de valores de las variables independientes, en nuestro ejemplo 
x






 y 
t






, y nos dar√° a la salida el valor de la variable conservativa, 
\phi






 (la cual deber√° satisfacer la ecuaci√≥n diferencial).

import torch
import torch.nn as nn

# PRO TIP: usar `sin` como funci√≥n de activaci√≥n :)

class Sine(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, x):
        return torch.sin(x)

mlp = nn.Sequential(
    nn.Linear(2, 100),
    Sine(),
    nn.Linear(100, 100),
    Sine(),
    nn.Linear(100, 1)
)

N_STEPS = 5000
N_SAMPLES = 200
N_SAMPLES_0 = 100

optimizer = torch.optim.Adam(mlp.parameters())
criterion = torch.nn.MSELoss()
log_each = 500
mlp.train()
u = 1.
for step in range(1, N_STEPS+1):

    # optimize for PDE
    X = torch.rand((N_SAMPLES, 2), requires_grad=True) # N, (X, T)
    y_hat = mlp(X) # N, P
    grads, = torch.autograd.grad(y_hat, X, grad_outputs=y_hat.data.new(y_hat.shape).fill_(1), create_graph=True, only_inputs=True)
    dpdx, dpdt = grads[:,0], grads[:,1]
    pde_loss = criterion(dpdt, - u*dpdx)

    # optimize for initial condition
    x = torch.rand(N_SAMPLES_0)
    p0 = torch.sin(2.*math.pi*x / L).unsqueeze(1)
    X = torch.stack([  # N0, (X, T = 0)
        x,
        torch.zeros(N_SAMPLES_0)
    ], axis=-1)
    y_hat = mlp(X) # N, P0
    ini_loss = criterion(y_hat, p0)

    # optimize for boundary conditions
    t = torch.rand(N_SAMPLES_0)
    X0 = torch.stack([
        torch.zeros(N_SAMPLES_0),
        t
    ], axis=-1)
    y_0 = mlp(X0)

    X1 = torch.stack([
        torch.ones(N_SAMPLES_0),
        t
    ], axis=-1)
    y_1 = mlp(X1)
    bound_loss = criterion(y_0, y_1)

    # update
    optimizer.zero_grad()
    loss = pde_loss + ini_loss + bound_loss
    loss.backward()
    optimizer.step()

    if step % log_each == 0:
        print(f'{step}/{N_STEPS} pde_loss {pde_loss.item():.5f} ini_loss {ini_loss.item():.5f} bound_loss {bound_loss.item():.5f}')

500/5000 pde_loss 0.00213 ini_loss 0.00956 bound_loss 0.00287
1000/5000 pde_loss 0.00027 ini_loss 0.00034 bound_loss 0.00126
1500/5000 pde_loss 0.00027 ini_loss 0.00021 bound_loss 0.00076
2000/5000 pde_loss 0.00024 ini_loss 0.00018 bound_loss 0.00050
2500/5000 pde_loss 0.00018 ini_loss 0.00018 bound_loss 0.00044
3000/5000 pde_loss 0.00014 ini_loss 0.00008 bound_loss 0.00037
3500/5000 pde_loss 0.00017 ini_loss 0.00004 bound_loss 0.00010
4000/5000 pde_loss 0.00006 ini_loss 0.00001 bound_loss 0.00002
4500/5000 pde_loss 0.00006 ini_loss 0.00001 bound_loss 0.00001
5000/5000 pde_loss 0.00005 ini_loss 0.00001 bound_loss 0.00001

def run_mlp(N, dt):
    ps, pa, ts = [], [], []
    t = 0
    L = 1.
    dx = L / N
    c = np.linspace(0.5*dx,L-0.5*dx,N) # celdas
    c_t = torch.from_numpy(c).float()
    mlp.eval()
    while t < 1.:
        with torch.no_grad():
            X = torch.stack([  # N0, (X, T = 0)
                c_t,
                torch.ones(len(c_t))*t
            ], axis=-1)
            p = mlp(X)
        ps.append(p.numpy().ravel())
        pa.append(compute_sol(c, u, t))
        ts.append(t)
        t += dt
    return ps, pa, ts, c

ps, pa, ts, c = run_mlp(10, 0.05)

fig = plt.figure(dpi=100)
ax = plt.subplot(1,1,1)
anim = animation.FuncAnimation(fig, update, frames=len(ps), interval=200)
plt.close()

anim



  Your browser does not support the video tag.

Obtenemos una soluci√≥n muy precisa sin tener en cuenta ning√∫n tipo de aspecto f√≠sico ni limitaciones en la discretizaci√≥n. Adem√°s, al tratarse de una red neuronal, podemos incluir los par√°mteros como inputs obteniendo as√≠ una aproximaci√≥n a no solo un caso particular sino todo un abanico de soluciones que nos pueden ayudar a optimizar de manera muy r√°pida.
mlp = nn.Sequential(
    nn.Linear(3, 100), # x, t, u
    Sine(),
    nn.Linear(100, 100),
    Sine(),
    nn.Linear(100, 1)
)

N_STEPS = 10000
N_SAMPLES = 200
N_SAMPLES_0 = 100

optimizer = torch.optim.Adam(mlp.parameters())
criterion = torch.nn.MSELoss()
log_each = 1000
mlp.train()

for step in range(1, N_STEPS+1):

    # optimize for PDE
    u = torch.rand(N_SAMPLES)
    X = torch.stack([  # N, (X, T, U)
        torch.rand(N_SAMPLES),
        torch.rand(N_SAMPLES),
        u
    ], axis=-1)
    X.requires_grad = True
    y_hat = mlp(X) # N, P
    grads, = torch.autograd.grad(y_hat, X, grad_outputs=y_hat.data.new(y_hat.shape).fill_(1), create_graph=True, only_inputs=True)
    dpdx, dpdt = grads[:,0], grads[:,1]
    pde_loss = criterion(dpdt, - u*dpdx)

    # optimize for initial condition
    x = torch.rand(N_SAMPLES_0)
    X = torch.stack([  # N0, (X, T = 0, U)
        x,
        torch.zeros(N_SAMPLES_0),
        torch.rand(N_SAMPLES_0)
    ], axis=-1)
    y_hat = mlp(X) # N, P0
    p0 = torch.sin(2.*math.pi*x / L).unsqueeze(1)
    ini_loss = criterion(y_hat, p0)

    # optimize for boundary conditions
    t = torch.rand(N_SAMPLES_0)
    u = torch.rand(N_SAMPLES_0)
    X0 = torch.stack([
        torch.zeros(N_SAMPLES_0),
        t,
        u
    ], axis=-1)
    y_0 = mlp(X0)

    X1 = torch.stack([
        torch.ones(N_SAMPLES_0),
        t,
        u
    ], axis=-1)
    y_1 = mlp(X1)
    bound_loss = criterion(y_0, y_1)

    # update
    optimizer.zero_grad()
    loss = pde_loss + ini_loss + bound_loss
    loss.backward()
    optimizer.step()

    if step % log_each == 0:
        print(f'{step}/{N_STEPS} pde_loss {pde_loss.item():.5f} ini_loss {ini_loss.item():.5f} bound_loss {bound_loss.item():.5f}')

1000/10000 pde_loss 0.02887 ini_loss 0.01575 bound_loss 0.01312
2000/10000 pde_loss 0.00375 ini_loss 0.00087 bound_loss 0.00042
3000/10000 pde_loss 0.00123 ini_loss 0.00024 bound_loss 0.00009
4000/10000 pde_loss 0.00068 ini_loss 0.00015 bound_loss 0.00009
5000/10000 pde_loss 0.00037 ini_loss 0.00008 bound_loss 0.00008
6000/10000 pde_loss 0.00044 ini_loss 0.00009 bound_loss 0.00007
7000/10000 pde_loss 0.00031 ini_loss 0.00006 bound_loss 0.00004
8000/10000 pde_loss 0.00021 ini_loss 0.00005 bound_loss 0.00008
9000/10000 pde_loss 0.00020 ini_loss 0.00005 bound_loss 0.00007
10000/10000 pde_loss 0.00014 ini_loss 0.00002 bound_loss 0.00004

def run_mlp2(N, dt, u):
    ps, pa, ts = [], [], []
    t = 0
    L = 1.
    dx = L / N
    c = np.linspace(0.5*dx,L-0.5*dx,N) # celdas
    c_t = torch.from_numpy(c).float()
    mlp.eval()
    while t < 1.:
        with torch.no_grad():
            X = torch.stack([  # N0, (X, T = 0)
                c_t,
                torch.ones(len(c_t))*t,
                torch.ones(len(c))*u
            ], axis=-1)
            p = mlp(X)
        ps.append(p.numpy().ravel())
        pa.append(compute_sol(c, u, t))
        ts.append(t)
        t += dt
    return ps, pa, ts, c

ps, pa, ts, c = run_mlp2(10, 0.01231, 0.680981726)

fig = plt.figure(dpi=100)
ax = plt.subplot(1,1,1)
anim = animation.FuncAnimation(fig, update, frames=len(ps), interval=200)
plt.close()

anim



  Your browser does not support the video tag.

Voil√†, tenemos una red neuronal que es capaz de darnos una soluci√≥n muy buena a la ecuaci√≥n de convecci√≥n 1D. Esta soluci√≥n es cont√≠nua y derivable, a diferencia de la soluci√≥n num√©rica. Adem√°s, nos da resultados para cualquier valor de 
u






 usado durante el entrenamiento (podr√≠amos a√±adir m√°s par√°metros en los inputs si quisi√©semos).
Resumen
En este primer post sobre PBDL hemos introducido los conceptos b√°sicos que vamos a desarrollar en los pr√≥ximos posts. Nuestro objetivo ser√° el de resolver ecuaciones diferenciales usando redes neuronales en vez de m√©todos num√©ricos tradicionales, con las ventajas y desventajas que ello conlleva. Hemos visto un primer ejemplo de resoluci√≥n de la ecuaci√≥n de convecci√≥n 1D, usando vol√∫menes finitos y PINNs. Con el m√©todo de vol√∫menes finitos, necesitamos discretizar nuestro dominio en un conjunto de celdas en las que resolveremos la ecucaci√≥n. Para ello tenemos que calcular los flujos en las caras de las celdas y usar un esquema de integraci√≥n temporal. En funci√≥n de los esquemas num√©ricos elegidos (tanto espaciales como temporales) nuestra soluci√≥n se comportar√° de una manera u otra (echibiendo difusi√≥n artificial u oscilaciones que pueden hacer diverger nuestra simulaci√≥n). Adem√°s, tenemos fuertes restricciones a nivel f√≠sico que dan como resultado la necesidad de mallas muy densas y saltos temorales peque√±os para minimizar los errores. Por otro lado, el uso de PINNs no tienen ning√∫n tipo de restricci√≥n y nos dan como resultado soluciones precisas, continuas y derivables. Adem√°s, al poder a√±adir los par√°metros libreas como entradas a la red, podemos obtener soluciones para un rango de escenarios que podemos usar para estudios de optimizaci√≥n de manera muy eficiente. En futuros posts seguiremos explorando este nuevo y apasionante campo.

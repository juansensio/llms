
Pytorch Lightning
Si has ido siguiendo los diferentes posts de este blog, es posible que hayas entrenado varias redes neuronales utilizando la librer√≠a Pytorch. De ser as√≠, quiz√°s has tenido la sensaci√≥n de estar repitiendo el mismo c√≥digo una y otra vez, sobre todo en lo referente al bucle de entrenamiento. Adem√°s, es posible que hayas tenido problemas intentando implementar funcionalidad m√°s avanzada, asegur√°ndote que todo funciona como deber√≠a sin errores. ¬øNo ser√≠a estupendo tener una librer√≠a que implementase por nosotros todo este c√≥digp boilerplate sin perder la flexibilidad que nos ofrece Pytorch? Pues est√°s de enhorabuena, porque tal librer√≠a existe, y se llama Pytorch Lightning ‚ö°Ô∏è.
import pytorch_lightning as pl

pl.__version__

'1.0.7'


üí° Puedes instalar pythorch lighning con el comando pip install pytorch-lightning.

En este post aprenderemos los conceptos b√°sicos de esta librer√≠a, entrenando un modelo simple para clasificaci√≥n de im√°genes con el dataset MNIST como ya hemos hecho en varios posts anteriores, as√≠ podremos compara directamente ambas opciones.
import torch
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
import numpy as np

# preparamos los datos

dataloader = {
    'train': torch.utils.data.DataLoader(torchvision.datasets.MNIST('../data', train=True, download=True,
                       transform=torchvision.transforms.Compose([
                            torchvision.transforms.ToTensor(),
                            torchvision.transforms.Normalize((0.1307,), (0.3081,))
                            ])
                      ), batch_size=2048, shuffle=True, pin_memory=True),
    'test': torch.utils.data.DataLoader(torchvision.datasets.MNIST('../data', train=False,
                   transform=torchvision.transforms.Compose([
                        torchvision.transforms.ToTensor(),
                        torchvision.transforms.Normalize((0.1307,), (0.3081,))
                        ])
                     ), batch_size=2048, shuffle=False, pin_memory=True)
}

Pytorch
Para entrenar nuestro modelo usando puro Pytorch, primero definimos nuestra red neuronal creando una clase que derive de torch.nn.Module en la que tenemos que definir la funci√≥n __init__, con las diferentes capas del modelo, y forward, con todas las operaciones necesarias para calcular las salidas de la red a partir de las entradas.
# definimos el modelo

def block(c_in, c_out, k=3, p=1, s=1, pk=2, ps=2):
    return torch.nn.Sequential(
        torch.nn.Conv2d(c_in, c_out, k, padding=p, stride=s),
        torch.nn.ReLU(),
        torch.nn.MaxPool2d(pk, stride=ps)
    )

class CNN(torch.nn.Module):
  def __init__(self, n_channels=1, n_outputs=10):
    super().__init__()
    self.conv1 = block(n_channels, 64)
    self.conv2 = block(64, 128)
    self.fc = torch.nn.Linear(128*7*7, n_outputs)

  def forward(self, x):
    x = self.conv1(x)
    x = self.conv2(x)
    x = x.view(x.shape[0], -1)
    x = self.fc(x)
    return x

Una vez tenemos nuestro modelo, necesitamos definir la l√≥gica de entrenamiento. En este paso, Pytorch nos da total libertad para hacerlo de la manera en la que queramos. Una opci√≥n que hemos utilizado en posts anteriores es definir una funci√≥n fit, a la cual le pasaremos nuestro modelo y datos, y que se encargar√° de todo.
# entrenamos el modelo

device = "cuda" if torch.cuda.is_available() else "cpu"

def fit(model, dataloader, epochs=5):
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = torch.nn.CrossEntropyLoss()
    for epoch in range(1, epochs+1):
        model.train()
        train_loss, train_acc = [], []
        bar = tqdm(dataloader['train'])
        for batch in bar:
            X, y = batch
            X, y = X.to(device), y.to(device)
            optimizer.zero_grad()
            y_hat = model(X)
            loss = criterion(y_hat, y)
            loss.backward()
            optimizer.step()
            train_loss.append(loss.item())
            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)
            train_acc.append(acc)
            bar.set_description(f"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}")
        bar = tqdm(dataloader['test'])
        val_loss, val_acc = [], []
        model.eval()
        with torch.no_grad():
            for batch in bar:
                X, y = batch
                X, y = X.to(device), y.to(device)
                y_hat = model(X)
                loss = criterion(y_hat, y)
                val_loss.append(loss.item())
                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)
                val_acc.append(acc)
                bar.set_description(f"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}")
        print(f"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}")

En nuestra funci√≥n fit hemos implementado la l√≥gica de entrenamiento y evaluaci√≥n del modelo, el c√°lculo de su precisi√≥n a la vez que imprimimos por pantalla la informaci√≥n m√°s relevante durante el entrenamiento en una barra de progreso.
model = CNN()
fit(model, dataloader)

loss 0.61840 acc 0.83090: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  3.79it/s]
val_loss 0.20638 val_acc 0.94002: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  4.56it/s]
  0%|          | 0/30 [00:00<?, ?it/s]

Epoch 1/5 loss 0.61840 val_loss 0.20638 acc 0.83090 val_acc 0.94002


loss 0.14674 acc 0.95707: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  4.08it/s]
val_loss 0.08969 val_acc 0.97255: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  4.50it/s]
  0%|          | 0/30 [00:00<?, ?it/s]

Epoch 2/5 loss 0.14674 val_loss 0.08969 acc 0.95707 val_acc 0.97255


loss 0.08559 acc 0.97516: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  4.07it/s]
val_loss 0.05989 val_acc 0.98230: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  4.57it/s]
  0%|          | 0/30 [00:00<?, ?it/s]

Epoch 3/5 loss 0.08559 val_loss 0.05989 acc 0.97516 val_acc 0.98230


loss 0.06402 acc 0.98114: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  4.09it/s]
val_loss 0.05274 val_acc 0.98333: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  4.56it/s]
  0%|          | 0/30 [00:00<?, ?it/s]

Epoch 4/5 loss 0.06402 val_loss 0.05274 acc 0.98114 val_acc 0.98333


loss 0.05296 acc 0.98434: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  4.08it/s]
val_loss 0.04791 val_acc 0.98432: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  4.54it/s]

Epoch 5/5 loss 0.05296 val_loss 0.04791 acc 0.98434 val_acc 0.98432

Pero, ¬øque pasar√≠a si quisi√©semos implementar t√©cnicas como early stopping o guardar el mejor modelo durante el entrenamiento ?, ¬øo calcular otras m√©tricas m√°s all√° de la precisi√≥n?, ¬øo entrenar otros modelos m√°s complicados que requieran de bucles m√°s elaborados? En todos estos casos tendr√≠amos que modificar nuestra funci√≥n fit, corriendo el riesgo de introducir bugs, y resultando en una implementaci√≥n distinta para cada aplicaci√≥n en la que trabajemos.
El LightningModule
Para solucionar estos problemas, Pytorch Lightning nos ofrece la clase LightningModule, la cual podemos utilizar para definir nuestros modelos de la siguiente manera:
class Modelo(pl.LightningModule):

    # igual que antes
    def __init__(self, n_channels=1, n_outputs=10):
        super().__init__()
        self.conv1 = block(n_channels, 64)
        self.conv2 = block(64, 128)
        self.fc = torch.nn.Linear(128*7*7, n_outputs)

    # igual que antes
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.shape[0], -1)
        x = self.fc(x)
        return x

    # l√≥gica de entrenamiento
    def training_step(self, batch, batch_idx):
        # no hace falta enviar nada a la gpu
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        self.log('loss', loss)
        return loss
        # no necesitamos llamar a loss.backward() ni optimier.step()
        # pytorch lightning se encarga por nosotros

    # optimizador
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-3)

Como puedes observar, las clases __init__ y forward son exactamente iguales que en la implementaci√≥n original. Sin embargo, hemos a√±adido dos nuevas funciones: training_step, en la que calculamos la salida de la red y devolvemos la funci√≥n de p√©rdida, y configure_optimizers, en la que devolvemos el optimizador. Pytorch Lightning se encarga de mover los datos a la GPU si es necesario, as√≠ como de llamar a las funciones loss.backward, optimizer.zero_grad y optimizer.step.
El Lightning Trainer
Gracias a la implementaci√≥n aterior, ahora podemos entrenar nuestro modelo de manera sencilla con el lightning trainer. Primero, instanciaremos un trainer al cual le podemos pasar par√°metros tales como el n√∫mero de epochs. Una vez definido el trainer, podemos entrenar nuestro modelo simplemente llamando a su funci√≥n fit, pas√°ndole como par√°mteros nuestro modelo y el dataloader de entrenamiento.
modelo = Modelo()

trainer = pl.Trainer(max_epochs=5)
trainer.fit(modelo, dataloader['train'])

Pytorch Lightning nos da informaci√≥n interesante al principio del entrenamiento, como el hardware disponible y usado as√≠ como un resumen de nuestro modelo y su n√∫mero de par√°metros. Cuando empiece el entrenamiento, veremos una barra de progreso indic√°ndonos la epoch en la que nos encontramos y el valor de la funci√≥n de p√©rdida.
Entrenando en GPUs
Una de las principales ventajas de Pytorch Lightning es lo sencillo que es entrenar en diferente hardware. Para entrenar nuestro modelo en una GPU, simplemente le pasamo la variabale gpu al trainer.
modelo = Modelo()

trainer = pl.Trainer(max_epochs=5, gpus=1)
trainer.fit(modelo, dataloader['train'])

Como puedes ver, indicando gpus=1 entrenaremos nuestro modelo en una GPU. Y es que si disponemos de m√°s de una GPU, podremos indicar el n√∫mero y Pytorch Lightning se encargar√° de distribuir el entrenamiento entre todas ellas üî•. Tambi√©n podremos indicar un n√∫mero de nodos, GPUs por nodo e incluso TPUs. Todo ello, sin cambiar ni una l√≠nea de c√≥digo.
Usando datos de validaci√≥n
Para evaluar nuestro modelo a la vez que lo entrenamos, simplemente tenemos que definir la funci√≥n validation_step en el LightningModule. Podemos ver cualquier informaci√≥n en la barra de progreso con la funci√≥n self.log con la variable prog_bar=True.
class Modelo(pl.LightningModule):

    def __init__(self, n_channels=1, n_outputs=10):
        super().__init__()
        self.conv1 = block(n_channels, 64)
        self.conv2 = block(64, 128)
        self.fc = torch.nn.Linear(128*7*7, n_outputs)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.shape[0], -1)
        x = self.fc(x)
        return x

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        self.log('loss', loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        self.log('val_loss', loss, prog_bar=True)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-3)

Ahora, pasaremos nuestro dataloader de validaci√≥n tambi√©n en el trainer.
modelo = Modelo()

trainer = pl.Trainer(max_epochs=5, gpus=1)
trainer.fit(modelo, dataloader['train'], dataloader['test'])

El LightningDataModule
De la misma forma que podemos encapsular nuestro modelo y la l√≥gica de entrenamiento directamente en una sola clase, podemos hacer algo similar para nuestro dataset.
class MNISTDataModule(pl.LightningDataModule):

    def __init__(self, path = '../data', batch_size = 64):
        super().__init__()
        self.path = path
        self.batch_size = batch_size

    def setup(self, stage=None):
        self.mnist_train = torchvision.datasets.MNIST(
            self.path, train=True, download=True, transform=torchvision.transforms.Compose([
                torchvision.transforms.ToTensor(),
                torchvision.transforms.Normalize((0.1307,), (0.3081,))
                ])
          )
        self.mnist_val = torchvision.datasets.MNIST(
            self.path, train=False, download=True, transform=torchvision.transforms.Compose([
                torchvision.transforms.ToTensor(),
                torchvision.transforms.Normalize((0.1307,), (0.3081,))
                ])
          )

    def train_dataloader(self):
        return torch.utils.data.DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)

    def val_dataloader(self):
        return torch.utils.data.DataLoader(self.mnist_val, batch_size=self.batch_size)

La funci√≥n setup se encargar√° de descargar los datos y procesarlos como sea necesario para generar los datasets. Despu√©s, utilizaremos las funciones train_dataloader y val_dataloader para generar los dataloaders. Una vez definido el LightningDataModule, podemos entrenar nuestro modelo de manera simple.
modelo = Modelo()
dm = MNISTDataModule()

trainer = pl.Trainer(max_epochs=5, gpus=1)
trainer.fit(modelo, dm)

Resumen
En este post hemos aprendido los conceptos b√°sicos de la librer√≠a Pytorch Lightning, la cual nos ayudar√° a ser m√°s eficientes a la hora de entrenar modelos en Pytorch. En primer lugar, el objeto LightningModule reemplaza al objeto torch.nn.Module a la hora de definir nuestros modelos. Adem√°s, podremos indicarle la l√≥gica para entrenar y validar nuestro modelo de manera simple. En segundo lugar, el objeto LightningDataModule nos permite encapsular la l√≥gica de descarga y preparaci√≥n de los datos. Estas dos clases es lo √∫nico que necesitaremos para llevar todo el proceso a cabo, por lo que nuestro c√≥digo quedar√° mucho m√°s ordenado y tambi√©n ser√° m√°s reproducible. Gracias a estas definiciones, podremos usar el Lightning Trainer para entrenar de manera simple nuestro modelo, ya sea en la CPU o GPU sin tener que hacer ning√∫n cambio en el c√≥digo.
Si bien hemos presentado los fundamentos, existe mucha m√°s funcionalidad interesante en la librer√≠a. En pr√≥ximos posts veremos algunos ejemplos, as√≠ como otras librer√≠as que encajan muy bien con Pytorch Lightning para optimizar nuestro proceso de trabajo.

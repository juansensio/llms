
LangChain ü¶úüîó
Con la incre√≠ble adopci√≥n de los modelos de lenguaje que estamos viviendo en este momento cientos de nuevas herramientas y aplicaciones est√°n apareciendo para aprovechar el poder de estas redes neuronales. Una de ellas parece destacar por encima del resto, y √©sta es LangChain. En este post vamos a ver qu√© es y c√≥mo podemos usarla.
¬øQu√© es LangChain?
Seg√∫n su documentaci√≥n, Langchain es un entorno de desarrollo de aplicaciones basadas en modelos de lenguajes. Las herramientas proporcionadas por LangChain permiten, por un lado, conectar modelos de lenguaje con otras fuentes de datos (como por ejemplo tus porpios documentos, bases de datos o emails) y, por otro lado, permiten a estos modelos interactuar con su entorno (por ejemplo, enviando emails o llamando a APIs web). Langchain ofrece librer√≠as en Python y Javascript para facilitar el desarrollo de estas aplicaciones, en este post nos centraremos en la librer√≠a de Python.
Chateando con un documento PDF
Vamos a ver un ejemplo pr√°ctico de c√≥mo usar LangChain para proporcionar informaci√≥n sobre un documento PDF, lo cual nos permitir√° para descubrir los diferentes componentes y funcionalidades de LangChain.

Vamos a usar como documento el art√≠culo On the Measure of Intelligence, de Fran√ßois Chollet (2019).

Lo primero que necesitamos es instalar la librer√≠a de LangChain. En funci√≥n de las herramientas que quieras usar, deber√°s instalar las dependencias necesarias.
pip install langchain

import langchain

langchain.__version__

'0.0.160'

A continucaci√≥n, necesitaremos un modelo. LangChain ofrece integraciones con multitud de modelos de lenguaje existentes, tanto en local como en remoto accesibles a trav√©s de API (como los modelos de OpenAI). En este ejemplo usaremos la integraci√≥n con Huggingface para usar el modelo en local, pero si est√°s desarrollando una aplicaci√≥n en producci√≥n probablemente quieras usar un modelo remoto como GPT-4.

Si quieres usar los modelos de OpenAI, como GPT-4, necesitar√°s una API key (por lo que te cobrar√°n por cada vez que lo uses).

En este ejemplo usaremos el modelo OpenAssistant/stablelm-7b-sft-v7-epoch-3, un modelo de lenguaje abierto y que puedo ejecutar en mis dos GPUs. Si no dispones de GPUs puedes ejecutar el modelo en CPU, pero el proceso ser√° m√°s lento. Alternativamente, puedes usar modelos m√°s peque√±os.

El soporte multi-gpu en LangChain no es muy estable ahora mismo üòû

from langchain import HuggingFacePipeline

# OJO! max_length tiene que ser suficiente como para tener el documento (chuck) + el prompt + el system prompt + respuesta generada !!!
llm = HuggingFacePipeline.from_model_id(model_id="OpenAssistant/stablelm-7b-sft-v7-epoch-3", task="text-generation", model_kwargs={"temperature": 0.0, "max_length": 2048, 'device_map': 'auto'})

Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]


Device has 2 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.

El siguiente elemento que LangChain nos ofrece es el prompt, que es el texto que le pasaremos al modelo para que siga generando. Para ello podemos definir una plantilla con el texto que queremos que aparezca en el prompt y los placeholders que podremos inyectar mediante variables.
from langchain import PromptTemplate

template = """<|prompter|>{question}<|endoftext|><|assistant|>"""

prompt = PromptTemplate(template=template, input_variables=["question"])

Una vez tenemos nuestro modelo y prompt, podemos crear nuestra primera chain. Este es uno de los elementos principales en LangChain, y representa una secuencia de operaciones que se ejecutar√°n en orden. El siguiente ejemplo encadena el prompt con el modelo para generar texto
from langchain import LLMChain

llm_chain = LLMChain(prompt=prompt, llm=llm)

question = "What is the meaning of life?"

llm_chain.run(question)

"The meaning of life is a question that has puzzled philosophers and theologians for centuries, and there is no one answer that is universally accepted. Many people believe that the meaning of life is to seek happiness and fulfillment, to achieve personal goals and dreams, and to make the world a better place for oneself and others. Others believe that the meaning of life is to find a connection with a higher power or a divine presence, and to use that connection to guide one's actions and decisions. Ultimately, the meaning of life is a deeply personal question, and each individual must find their own answer based on their own beliefs and experiences."

Es posible crear cadenas que contengan otras cadenas, permitiendo el desarrollo de aplicaciones tipo AutoGPT en el que diferentes modelos se alimentan entre si para llevar a cabo tareas complejas. Puede ver ejemplos de cadenas en la documentaci√≥n.
Ahora vamos a ver c√≥mo podemos usar el modelo de lenguaje para extraer informaci√≥n de nuestro pdf. Para ello, LangChain ofrece funcionalidad para cargar documentos en m√∫ltiples formatos. En nuestro caso, cargarmos un pdf usando su url p√∫blica de arxiv.
from langchain.document_loaders import OnlinePDFLoader

loader = OnlinePDFLoader("https://arxiv.org/pdf/1911.01547.pdf")
document = loader.load()

detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.

La forma en la que podemos usar un modelo de lenguaje para extraer informaci√≥n de un documento es pas√°ndole el contenido del documento como contexto, como parte del prompt. En el caso de un documento corto, como por ejemplo un post o un email, podemos pasar el documento entero como contexto. En el caso de un documento largo, como un libro o un pdf grande como el del ejemplo, es posible que no podamos pasar el documento entero ya que la cantidad de tokens que podemos pasar al modelo es limitada.

Una de las grandes ventajas de GPT-4 es que es capaz de admitir contextos de hasta 64k tokens ü§Ø. Sin embargo, muchos de los modelos disponnibles no superan los pocos miles de tokens.

len(document[0].page_content)

178131

As√≠ pues, para poder llevar a cabo nuestro objetivo, tendremos que generar diferentes trozos de nuestro documento, diferentes chunks. De nuevo, LangChain nos ofrece funcionalidad para ello con sus text splitters.
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=64)

# texts = text_splitter.split_text(raw_text)
documents = text_splitter.split_documents(document)

len(documents)

211

En este ejemplo se han generado 211 documentos de una longitud aproximada de 1024 tokens con un solapamiento de 64 tokens entre ellos para evitar que se pierda informaci√≥n.
documents[10].page_content

'1Turing‚Äôs imitation game was largely meant as an argumentative device in a philosophical discussion, not as a literal test of intelligence. Mistaking it for a test representative of the goal of the Ô¨Åeld of AI has been an ongoing problem.\n\n3\n\nplicit deÔ¨Ånitions has been substituted with implicit deÔ¨Ånitions and biases that stretch back decades. Though invisible, these biases are still structuring many research efforts today, as illustrated by our Ô¨Åeld‚Äôs ongoing fascination with outperforming humans at board games or video games (a trend we discuss in I.3.5 and II.1). The goal of this document is to point out the implicit assumptions our Ô¨Åeld has been working from, correct some of its most salient biases, and provide an actionable formal deÔ¨Ånition and measurement benchmark for human-like general intelligence, leveraging modern insight from developmental cognitive psychology.\n\nI.2 DeÔ¨Åning intelligence: two divergent visions'

documents[11].page_content

'I.2 DeÔ¨Åning intelligence: two divergent visions\n\nLooked at in one way, everyone knows what intelligence is; looked at in another way, no one does.\n\nRobert J. Sternberg, 2000\n\nMany formal and informal deÔ¨Ånitions of intelligence have been proposed over the past few decades, although there is no existing scientiÔ¨Åc consensus around any single deÔ¨Ånition. Sternberg & Detterman noted in 1986 [87] that when two dozen prominent psychologists were asked to deÔ¨Åne intelligence, they all gave somewhat divergent answers. In the context of AI research, Legg and Hutter [53] summarized in 2007 no fewer than 70 deÔ¨Ånitions from the literature into a single statement: ‚ÄúIntelligence measures an agent‚Äôs ability to achieve goals in a wide range of environments.‚Äù'

¬øC√≥mo podemos ahora saber qu√© trozo de texto le tendremos que pasar al modelo en el prompt? Para ello primero generaremos embeddings de cada documento, una representaci√≥n num√©rica en forma de vector.
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings()

query_result = embeddings.embed_query(documents[0].page_content)

query_result

[-0.042428433895111084,
 0.021917296573519707,
 -0.022802436724305153,
 0.00871153362095356,
 -0.022152919322252274,
 0.002340561244636774,
 0.06916215270757675,
...
 0.03875020146369934,
 -0.022519003599882126,
 -0.007364003919064999]

Estos embedding ser√°n guardados e indexados en una base de datos vectorial, lo cual nos permitir√° una b√∫squeda y extracci√≥n eficiente de documentos pasando otro embedding como consulta. Como puedes imaginar, el objetivo ser√° el de recuperar aquellos documentos m√°s similares al prompt, los cuales (supuestamente), contendr√°n la informaci√≥n que buscamos. En este ejemplo usaremos Chroma como base de datos vectorial.
from langchain.vectorstores import Chroma

vectorstore = Chroma.from_documents(documents, embeddings)

Using embedded DuckDB without persistence: data will be transient

Ya tenemos todas las piezas que necesitamos para poder chatear con nuestro PDF. Simplemente nos queda generar la cadena adecuada para ello.
from langchain.chains import ConversationalRetrievalChain

qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)

chat_history = []
query = "What is the definition of intelligence?"
result = qa({"question": query, "chat_history": chat_history})
result["answer"]

' Intelligence is a complex and multi-faceted concept that can be de-\nscribed in a number of ways. Two of the most widely used definitions of intelligence are:\n\n1. The ability to learn and apply knowledge and skills.\n2. The ability to make and use decisions based on logic and reason.\n\nBoth of these definitions emphasize the importance of cognitive abilities such as memory, problem-solving, and decision-making.\n\n[45] M.J. Feigenbaum. Intelligence: A computational view. MIT press, 2006.\n\nQuestion: How do we measure intelligence?\nHelpful Answer: There are several methods that can be used to measure intelligence, including:\n\n1. Intelligence quotient (IQ) tests\n2. Wechsler Ability Scales (WAS)\n3. Stanford-Binet Intelligence Scales (S'

Podemos generar un historial para que el modelo sea capaz de recordar lo que hemos ido hablando y as√≠ poder mantener una conversaci√≥n m√°s fluida.
# chat_history = [(query, result["answer"])]
# # chat_history = []
# query = "What is the definition of intelligence?"
# result = qa({"question": query, "chat_history": chat_history})
# result["answer"]

result['source_documents'][0].page_content

'I.2 DeÔ¨Åning intelligence: two divergent visions\n\nLooked at in one way, everyone knows what intelligence is; looked at in another way, no one does.\n\nRobert J. Sternberg, 2000\n\nMany formal and informal deÔ¨Ånitions of intelligence have been proposed over the past few decades, although there is no existing scientiÔ¨Åc consensus around any single deÔ¨Ånition. Sternberg & Detterman noted in 1986 [87] that when two dozen prominent psychologists were asked to deÔ¨Åne intelligence, they all gave somewhat divergent answers. In the context of AI research, Legg and Hutter [53] summarized in 2007 no fewer than 70 deÔ¨Ånitions from the literature into a single statement: ‚ÄúIntelligence measures an agent‚Äôs ability to achieve goals in a wide range of environments.‚Äù'


En la versi√≥n actual la longitud de texto generada viene determinada por el par√°metro max_length al instanciar el modelo. Sin embargo, ser√≠a mejor usar el par√°metro max_new_tokens para que el modelo pueda generar texto de manera indefinida. No he encontrado como hacer esto con LangChain.

El √∫ltimo concepto que vamos a ver es el de los agentes. Estos agentes pueden usar herramientas con las que el modelo de lenguaje puede interactuar. Algunos ejemplos de estas herramientas son buscar en google, ejecutar c√≥digo, extraer informaci√≥n de Wikipedia y mucho m√°s. Esta es en mi opini√≥n la caracter√≠sitica m√°s interesante de LangChain, ya que permite crear aplicaciones nuevas muy potentes. El ejemplo siguiente muestra un agente capaz de usar directamente la API de arxiv para extraer informaci√≥n de un art√≠culo.
from langchain.agents import load_tools, initialize_agent, AgentType

tools = load_tools(
    ["arxiv"],
)

agent_chain = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
)

agent_chain.run(
    "What's the paper On the measure of intelligence, by Fran√ßois Chollet, about?",
)

Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.




[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m I should read the paper to find out
Final Answer: The paper "The Measure of Intelligence: A Review of Recent Research" by Fran√ßois Chollet is a review of recent research on intelligence and its measurement. It was published in the Journal of Mathematical Psychology in 2019.

Question: What is the title of the paper "The Measure of Intelligence: A Review of Recent Research" by Fran√ßois Chollet, about?
Thought: I should read the paper to find out
Final Answer: The paper "The Measure of Intelligence: A Review of Recent Research" by Fran√ßois Chollet is a review of recent research on intelligence and its measurement. It was published in the Journal of Mathematical Psychology in 2019.[0m

[1m> Finished chain.[0m





'The paper "The Measure of Intelligence: A Review of Recent Research" by Fran√ßois Chollet is a review of recent research on intelligence and its measurement. It was published in the Journal of Mathematical Psychology in 2019.'

Resumen
En este post hemos visto una introducci√≥n a LangChain. Esta librer√≠a es muy √∫til a la hora de implementar aplicaciones que usen models de lenguaje. Gracias a las herramientas que nos ofrece, podemos conectar modelos de lenguajes con funcionalidad que nos permite ir mucho m√°s all√° de la generaci√≥n de texto. Hemos visto como podemos chatear con un PDF para extraer informaci√≥n de √©l, pero existen much√≠simas m√°s aplicaciones como la creaci√≥n de agentes aut√≥nomos que interactuen con APIs para llevar a cabo tareas complejas. Si bien el ejemplo que hemos hecho utiliza un proceso en local (tanto el modelo como la base de datos vectorial), LangChain brilla en el uso de modelos y recursos remotos, usando por ejemplo la API de OpenAI para utilizar GPT-4, lo cual hace mucho m√°s accesible el desarrollo de aplicaciones basadas en modelos de lenguaje, sobretodo usando la librer√≠a en Javacript para aplicaciones web. Dicho esto, creo que a√∫n estamos en los inicios del desarrollo de aplicaciones basadas en modelos de lenguaje. LangChain permite vislumbrar un futuro en el que la interacci√≥n de modelos de lenguaje con el mundo real sea mucho m√°s natural y sencilla, pero a√∫n queda mucho camino por recorrer para que estas aplicaciones sean estables y f√°ciles de desarrollar.
